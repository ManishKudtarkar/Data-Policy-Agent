{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-hAu4c3ba_G",
        "outputId": "a8ed954e-6027-40af-e518-d2b2634db5ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (1.0.0).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/ealtman2019/ibm-transactions-for-anti-money-laundering-aml?dataset_version_number=8...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7.61G/7.61G [01:22<00:00, 98.5MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/ealtman2019/ibm-transactions-for-anti-money-laundering-aml/versions/8\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"ealtman2019/ibm-transactions-for-anti-money-laundering-aml\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fbcdcf0"
      },
      "source": [
        "# Task\n",
        "Load and inspect the `PS_20174392719_1491204439457_log.csv` and `Policy_Compliance_Dataset_Updated.csv` datasets, generate descriptive statistical summaries for both, and then suggest further analysis or machine learning tasks for anti-money laundering (AML) or compliance based on the initial data exploration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "537e7d1a"
      },
      "source": [
        "# ***italicized text***## Load and Inspect Datasets *italicized text*\n",
        "\n",
        "### Subtask:\n",
        "Load 'PS_20174392719_1491204439457_log.csv' and 'Policy_Compliance_Dataset_Updated.csv' into pandas DataFrames and display their first few rows, column names, and basic information to understand their structure and content.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4da35a7d"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires loading two CSV files into pandas DataFrames and then displaying their first few rows, column names, and a concise summary. I will use the `pandas` library for this and the `read_csv`, `head`, `columns`, and `info` methods.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f5f40c4",
        "outputId": "3c2032fb-0546-475e-e68e-1b167da3ad8c"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the transaction dataset\n",
        "df_transactions = pd.read_csv('/content/PS_20174392719_1491204439457_log.csv')\n",
        "\n",
        "print(\"### df_transactions - First 5 Rows ###\")\n",
        "print(df_transactions.head())\n",
        "print(\"\\n### df_transactions - Column Names ###\")\n",
        "print(df_transactions.columns)\n",
        "print(\"\\n### df_transactions - Info ###\")\n",
        "df_transactions.info()\n",
        "\n",
        "# Load the policy compliance dataset\n",
        "df_policy = pd.read_csv('/content/Policy_Compliance_Dataset_Updated.csv')\n",
        "\n",
        "print(\"\\n### df_policy - First 5 Rows ###\")\n",
        "print(df_policy.head())\n",
        "print(\"\\n### df_policy - Column Names ###\")\n",
        "print(df_policy.columns)\n",
        "print(\"\\n### df_policy - Info ###\")\n",
        "df_policy.info()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### df_transactions - First 5 Rows ###\n",
            "   step      type    amount     nameOrig  oldbalanceOrg  newbalanceOrig  \\\n",
            "0     1   PAYMENT   9839.64  C1231006815       170136.0       160296.36   \n",
            "1     1   PAYMENT   1864.28  C1666544295        21249.0        19384.72   \n",
            "2     1  TRANSFER    181.00  C1305486145          181.0            0.00   \n",
            "3     1  CASH_OUT    181.00   C840083671          181.0            0.00   \n",
            "4     1   PAYMENT  11668.14  C2048537720        41554.0        29885.86   \n",
            "\n",
            "      nameDest  oldbalanceDest  newbalanceDest  isFraud  isFlaggedFraud  \n",
            "0  M1979787155             0.0             0.0      0.0             0.0  \n",
            "1  M2044282225             0.0             0.0      0.0             0.0  \n",
            "2   C553264065             0.0             0.0      1.0             0.0  \n",
            "3    C38997010         21182.0             0.0      1.0             0.0  \n",
            "4  M1230701703             0.0             0.0      0.0             0.0  \n",
            "\n",
            "### df_transactions - Column Names ###\n",
            "Index(['step', 'type', 'amount', 'nameOrig', 'oldbalanceOrg', 'newbalanceOrig',\n",
            "       'nameDest', 'oldbalanceDest', 'newbalanceDest', 'isFraud',\n",
            "       'isFlaggedFraud'],\n",
            "      dtype='object')\n",
            "\n",
            "### df_transactions - Info ###\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1293904 entries, 0 to 1293903\n",
            "Data columns (total 11 columns):\n",
            " #   Column          Non-Null Count    Dtype  \n",
            "---  ------          --------------    -----  \n",
            " 0   step            1293904 non-null  int64  \n",
            " 1   type            1293904 non-null  object \n",
            " 2   amount          1293904 non-null  float64\n",
            " 3   nameOrig        1293904 non-null  object \n",
            " 4   oldbalanceOrg   1293903 non-null  float64\n",
            " 5   newbalanceOrig  1293903 non-null  float64\n",
            " 6   nameDest        1293903 non-null  object \n",
            " 7   oldbalanceDest  1293903 non-null  float64\n",
            " 8   newbalanceDest  1293903 non-null  float64\n",
            " 9   isFraud         1293903 non-null  float64\n",
            " 10  isFlaggedFraud  1293903 non-null  float64\n",
            "dtypes: float64(7), int64(1), object(3)\n",
            "memory usage: 108.6+ MB\n",
            "\n",
            "### df_policy - First 5 Rows ###\n",
            "   Employee_ID         Name  Working_Days  Target_Sales  Actual_Sales  \\\n",
            "0            1  Ahmed Tariq            21         14435         19470   \n",
            "1            2  Usman Iqbal            18          9998          9968   \n",
            "2            3  Fatima Khan            27          5162         11493   \n",
            "3            4   Zain Tariq            29          6974          8103   \n",
            "4            5  Ahmed Malik            25          8291          6076   \n",
            "\n",
            "   Customer_Satisfaction_Score Policy_Compliance  Low_Working_Days  \\\n",
            "0                          2.3                No             False   \n",
            "1                          3.0                No              True   \n",
            "2                          3.3                No             False   \n",
            "3                          2.8                No             False   \n",
            "4                          3.7                No             False   \n",
            "\n",
            "   Target_Not_Met  Low_Customer_Satisfaction  \\\n",
            "0           False                       True   \n",
            "1           False                       True   \n",
            "2           False                       True   \n",
            "3           False                       True   \n",
            "4            True                       True   \n",
            "\n",
            "                         Non_Compliance_Reason     Month  \n",
            "0                    Low Customer Satisfaction     March  \n",
            "1  Low Working Days, Low Customer Satisfaction  December  \n",
            "2                    Low Customer Satisfaction  February  \n",
            "3                    Low Customer Satisfaction       May  \n",
            "4    Target Not Met, Low Customer Satisfaction  February  \n",
            "\n",
            "### df_policy - Column Names ###\n",
            "Index(['Employee_ID', 'Name', 'Working_Days', 'Target_Sales', 'Actual_Sales',\n",
            "       'Customer_Satisfaction_Score', 'Policy_Compliance', 'Low_Working_Days',\n",
            "       'Target_Not_Met', 'Low_Customer_Satisfaction', 'Non_Compliance_Reason',\n",
            "       'Month'],\n",
            "      dtype='object')\n",
            "\n",
            "### df_policy - Info ###\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4000 entries, 0 to 3999\n",
            "Data columns (total 12 columns):\n",
            " #   Column                       Non-Null Count  Dtype  \n",
            "---  ------                       --------------  -----  \n",
            " 0   Employee_ID                  4000 non-null   int64  \n",
            " 1   Name                         4000 non-null   object \n",
            " 2   Working_Days                 4000 non-null   int64  \n",
            " 3   Target_Sales                 4000 non-null   int64  \n",
            " 4   Actual_Sales                 4000 non-null   int64  \n",
            " 5   Customer_Satisfaction_Score  4000 non-null   float64\n",
            " 6   Policy_Compliance            4000 non-null   object \n",
            " 7   Low_Working_Days             4000 non-null   bool   \n",
            " 8   Target_Not_Met               4000 non-null   bool   \n",
            " 9   Low_Customer_Satisfaction    4000 non-null   bool   \n",
            " 10  Non_Compliance_Reason        3396 non-null   object \n",
            " 11  Month                        4000 non-null   object \n",
            "dtypes: bool(3), float64(1), int64(4), object(4)\n",
            "memory usage: 293.1+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "228da59b"
      },
      "source": [
        "## Summarize Transaction Log Dataset\n",
        "\n",
        "### Subtask:\n",
        "Generate a descriptive statistical summary for the 'PS_20174392719_1491204439457_log.csv' dataset. This dataset appears to be a transaction log, commonly used for fraud detection and anti-money laundering (AML) analysis. We'll look for insights into transaction types, amounts, and any 'isFraud' or 'isFlaggedFraud' columns if they exist.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04b33fb8"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to generate descriptive statistical summaries for both numerical and categorical columns of `df_transactions` and then display value counts for specific columns as requested in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a729a4b9",
        "outputId": "5bde1a87-4eea-4a21-be21-05e40f4c91ab"
      },
      "source": [
        "print(\"\\n### df_transactions - Numerical Columns Descriptive Statistics ###\")\n",
        "print(df_transactions.describe())\n",
        "\n",
        "print(\"\\n### df_transactions - Categorical Columns Descriptive Statistics ###\")\n",
        "print(df_transactions.describe(include='object'))\n",
        "\n",
        "print(\"\\n### df_transactions - Value Counts for 'type' ###\")\n",
        "print(df_transactions['type'].value_counts())\n",
        "\n",
        "print(\"\\n### df_transactions - Value Counts for 'isFraud' ###\")\n",
        "print(df_transactions['isFraud'].value_counts())\n",
        "\n",
        "print(\"\\n### df_transactions - Value Counts for 'isFlaggedFraud' ###\")\n",
        "print(df_transactions['isFlaggedFraud'].value_counts())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### df_transactions - Numerical Columns Descriptive Statistics ###\n",
            "               step        amount  oldbalanceOrg  newbalanceOrig  \\\n",
            "count  1.293904e+06  1.293904e+06   1.293903e+06    1.293903e+06   \n",
            "mean   4.648671e+01  1.630862e+05   8.754029e+05    8.959591e+05   \n",
            "std    4.288048e+01  2.731327e+05   2.972176e+06    3.008551e+06   \n",
            "min    1.000000e+00  1.000000e-01   0.000000e+00    0.000000e+00   \n",
            "25%    1.600000e+01  1.286836e+04   0.000000e+00    0.000000e+00   \n",
            "50%    3.500000e+01  7.864536e+04   1.553100e+04    0.000000e+00   \n",
            "75%    4.400000e+01  2.187513e+05   1.305024e+05    1.714609e+05   \n",
            "max    1.360000e+02  1.000000e+07   3.893942e+07    3.894623e+07   \n",
            "\n",
            "       oldbalanceDest  newbalanceDest       isFraud  isFlaggedFraud  \n",
            "count    1.293903e+06    1.293903e+06  1.293903e+06       1293903.0  \n",
            "mean     9.867757e+05    1.118626e+06  1.196380e-03             0.0  \n",
            "std      2.304700e+06    2.416089e+06  3.456805e-02             0.0  \n",
            "min      0.000000e+00    0.000000e+00  0.000000e+00             0.0  \n",
            "25%      0.000000e+00    0.000000e+00  0.000000e+00             0.0  \n",
            "50%      1.301768e+05    2.240197e+05  0.000000e+00             0.0  \n",
            "75%      9.306014e+05    1.158112e+06  0.000000e+00             0.0  \n",
            "max      4.220740e+07    4.220740e+07  1.000000e+00             0.0  \n",
            "\n",
            "### df_transactions - Categorical Columns Descriptive Statistics ###\n",
            "            type    nameOrig    nameDest\n",
            "count    1293904     1293904     1293903\n",
            "unique         5     1293511      553408\n",
            "top     CASH_OUT  C457838482  C985934102\n",
            "freq      460441           2          98\n",
            "\n",
            "### df_transactions - Value Counts for 'type' ###\n",
            "type\n",
            "CASH_OUT    460441\n",
            "PAYMENT     434787\n",
            "CASH_IN     282538\n",
            "TRANSFER    107425\n",
            "DEBIT         8713\n",
            "Name: count, dtype: int64\n",
            "\n",
            "### df_transactions - Value Counts for 'isFraud' ###\n",
            "isFraud\n",
            "0.0    1292355\n",
            "1.0       1548\n",
            "Name: count, dtype: int64\n",
            "\n",
            "### df_transactions - Value Counts for 'isFlaggedFraud' ###\n",
            "isFlaggedFraud\n",
            "0.0    1293903\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1af3f0f9"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the descriptive statistics for `df_transactions` have been generated, I will generate descriptive statistical summaries for the `df_policy` dataset, including numerical and categorical columns, and display value counts for relevant categorical columns to understand the compliance policies, as per the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8da6cefe",
        "outputId": "87fb85d7-0c5c-472f-b53c-ed92b9df2291"
      },
      "source": [
        "print(\"\\n### df_policy - Numerical Columns Descriptive Statistics ###\")\n",
        "print(df_policy.describe())\n",
        "\n",
        "print(\"\\n### df_policy - Categorical Columns Descriptive Statistics ###\")\n",
        "print(df_policy.describe(include='object'))\n",
        "\n",
        "print(\"\\n### df_policy - Value Counts for 'Policy_Compliance' ###\")\n",
        "print(df_policy['Policy_Compliance'].value_counts())\n",
        "\n",
        "print(\"\\n### df_policy - Value Counts for 'Low_Working_Days' ###\")\n",
        "print(df_policy['Low_Working_Days'].value_counts())\n",
        "\n",
        "print(\"\\n### df_policy - Value Counts for 'Target_Not_Met' ###\")\n",
        "print(df_policy['Target_Not_Met'].value_counts())\n",
        "\n",
        "print(\"\\n### df_policy - Value Counts for 'Low_Customer_Satisfaction' ###\")\n",
        "print(df_policy['Low_Customer_Satisfaction'].value_counts())\n",
        "\n",
        "print(\"\\n### df_policy - Value Counts for 'Non_Compliance_Reason' ###\")\n",
        "print(df_policy['Non_Compliance_Reason'].value_counts())\n",
        "\n",
        "print(\"\\n### df_policy - Value Counts for 'Month' ###\")\n",
        "print(df_policy['Month'].value_counts())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### df_policy - Numerical Columns Descriptive Statistics ###\n",
            "       Employee_ID  Working_Days  Target_Sales  Actual_Sales  \\\n",
            "count  4000.000000   4000.000000   4000.000000    4000.00000   \n",
            "mean   2000.500000     22.412250  12497.124500   12942.75300   \n",
            "std    1154.844867      4.620675   4361.473733    5216.77862   \n",
            "min       1.000000     15.000000   5002.000000    4001.00000   \n",
            "25%    1000.750000     18.000000   8707.000000    8419.50000   \n",
            "50%    2000.500000     22.000000  12425.000000   12997.50000   \n",
            "75%    3000.250000     26.000000  16368.250000   17453.25000   \n",
            "max    4000.000000     30.000000  19996.000000   22000.00000   \n",
            "\n",
            "       Customer_Satisfaction_Score  \n",
            "count                  4000.000000  \n",
            "mean                      3.493850  \n",
            "std                       0.864352  \n",
            "min                       2.000000  \n",
            "25%                       2.800000  \n",
            "50%                       3.500000  \n",
            "75%                       4.200000  \n",
            "max                       5.000000  \n",
            "\n",
            "### df_policy - Categorical Columns Descriptive Statistics ###\n",
            "                   Name Policy_Compliance      Non_Compliance_Reason  Month\n",
            "count              4000              4000                       3396   4000\n",
            "unique              100                 2                          7     12\n",
            "top     Fatima Chaudhry                No  Low Customer Satisfaction  March\n",
            "freq                 59              3396                       1208    359\n",
            "\n",
            "### df_policy - Value Counts for 'Policy_Compliance' ###\n",
            "Policy_Compliance\n",
            "No     3396\n",
            "Yes     604\n",
            "Name: count, dtype: int64\n",
            "\n",
            "### df_policy - Value Counts for 'Low_Working_Days' ###\n",
            "Low_Working_Days\n",
            "False    2710\n",
            "True     1290\n",
            "Name: count, dtype: int64\n",
            "\n",
            "### df_policy - Value Counts for 'Target_Not_Met' ###\n",
            "Target_Not_Met\n",
            "False    2665\n",
            "True     1335\n",
            "Name: count, dtype: int64\n",
            "\n",
            "### df_policy - Value Counts for 'Low_Customer_Satisfaction' ###\n",
            "Low_Customer_Satisfaction\n",
            "True     2614\n",
            "False    1386\n",
            "Name: count, dtype: int64\n",
            "\n",
            "### df_policy - Value Counts for 'Non_Compliance_Reason' ###\n",
            "Non_Compliance_Reason\n",
            "Low Customer Satisfaction                                      1208\n",
            "Target Not Met, Low Customer Satisfaction                       576\n",
            "Low Working Days, Low Customer Satisfaction                     543\n",
            "Target Not Met                                                  322\n",
            "Low Working Days                                                310\n",
            "Low Working Days, Target Not Met, Low Customer Satisfaction     287\n",
            "Low Working Days, Target Not Met                                150\n",
            "Name: count, dtype: int64\n",
            "\n",
            "### df_policy - Value Counts for 'Month' ###\n",
            "Month\n",
            "March        359\n",
            "September    358\n",
            "May          347\n",
            "January      345\n",
            "June         343\n",
            "November     334\n",
            "August       334\n",
            "December     326\n",
            "October      322\n",
            "April        316\n",
            "July         309\n",
            "February     307\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a545fb6"
      },
      "source": [
        "## Suggested Further Analysis and Machine Learning Tasks\n",
        "\n",
        "Based on the initial data exploration of `df_transactions` (transaction log) and `df_policy` (policy compliance), here are some suggestions for further analysis and potential machine learning tasks:\n",
        "\n",
        "### For `df_transactions` (AML/Fraud Detection):\n",
        "\n",
        "1.  **Fraud Detection Model Development:**\n",
        "    *   **Binary Classification:** The `isFraud` column is a clear target variable. Develop a classification model (e.g., Logistic Regression, Random Forest, Gradient Boosting, Isolation Forest, or even Neural Networks) to predict fraudulent transactions. Given the highly imbalanced nature (`0.0`: 1,292,355 vs `1.0`: 1,548), techniques like SMOTE, ADASYN, or using appropriate evaluation metrics (Precision, Recall, F1-score, AUC-PR) will be crucial.\n",
        "    *   **Feature Engineering:** Create new features that might be indicative of fraud. Examples include:\n",
        "        *   Transaction frequency per customer (`nameOrig`, `nameDest`) within a time window.\n",
        "        *   Average transaction amount per customer.\n",
        "        *   Ratio of transaction amount to old balance (`amount / oldbalanceOrg`).\n",
        "        *   Difference between old and new balances for both origin and destination accounts.\n",
        "        *   Indicators for sudden changes in transaction behavior.\n",
        "    *   **Anomaly Detection:** Explore unsupervised anomaly detection techniques for transactions that might not be labeled as fraud but exhibit unusual patterns.\n",
        "\n",
        "2.  **Time-Series Analysis:** The `step` column indicates time steps. Analyze transaction patterns over time. Are there specific steps (days) where fraud is more prevalent? Are certain `type` of transactions more common at certain `step` values?\n",
        "\n",
        "3.  **Graph Analysis:** The `nameOrig` and `nameDest` columns represent transacting entities. Construct a transaction network/graph to identify suspicious clusters or highly connected nodes that might be involved in fraudulent activities. Graph Neural Networks could be a powerful tool here.\n",
        "\n",
        "4.  **Data Quality and Missing Values:** Investigate the reason for missing values in `oldbalanceOrg`, `newbalanceOrig`, `nameDest`, `oldbalanceDest`, `newbalanceDest`, `isFraud`, and `isFlaggedFraud` (1 row missing). While minimal, understanding why these are missing could be important.\n",
        "\n",
        "### For `df_policy` (Compliance Analysis):\n",
        "\n",
        "1.  **Compliance Prediction Model:**\n",
        "    *   **Binary Classification:** The `Policy_Compliance` column (`Yes`/`No`) can be used as a target variable. Predict whether an employee will be compliant or non-compliant based on features like `Working_Days`, `Target_Sales`, `Actual_Sales`, `Customer_Satisfaction_Score`, `Low_Working_Days`, `Target_Not_Met`, and `Low_Customer_Satisfaction`.\n",
        "    *   **Feature Importance:** Determine which factors (e.g., `Target_Sales`, `Customer_Satisfaction_Score`) are most influential in predicting policy compliance.\n",
        "\n",
        "2.  **Root Cause Analysis for Non-Compliance:**\n",
        "    *   **Multi-label Classification/Association Rules:** The `Non_Compliance_Reason` column contains multiple reasons (e.g., \"Low Working Days, Low Customer Satisfaction\"). Analyze common combinations of reasons. This could be framed as a multi-label classification problem or by using association rule mining to find frequently co-occurring non-compliance reasons.\n",
        "    *   **Impact of `Month`:** Investigate if there's a seasonal or monthly trend in policy compliance or specific non-compliance reasons. (e.g., are employees less compliant in certain months?)\n",
        "\n",
        "3.  **Employee Performance vs. Compliance:** Analyze the relationship between performance metrics (`Actual_Sales`, `Customer_Satisfaction_Score`) and `Policy_Compliance`. Do high-performing employees tend to be more or less compliant?\n",
        "\n",
        "4.  **Clustering Employees:** Cluster employees based on their working patterns, sales performance, and compliance behaviors to identify different segments and tailor interventions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78dde367"
      },
      "source": [
        "## Suggested Further Analysis and Machine Learning Tasks\n",
        "\n",
        "Based on the initial data exploration of `df_transactions` (transaction log) and `df_policy` (policy compliance), here are some suggestions for further analysis and potential machine learning tasks:\n",
        "\n",
        "### For `df_transactions` (AML/Fraud Detection):\n",
        "\n",
        "1.  **Fraud Detection Model Development:**\n",
        "    *   **Binary Classification:** The `isFraud` column is a clear target variable. Develop a classification model (e.g., Logistic Regression, Random Forest, Gradient Boosting, Isolation Forest, or even Neural Networks) to predict fraudulent transactions. Given the highly imbalanced nature (`0.0`: 1,292,355 vs `1.0`: 1,548), techniques like SMOTE, ADASYN, or using appropriate evaluation metrics (Precision, Recall, F1-score, AUC-PR) will be crucial.\n",
        "    *   **Feature Engineering:** Create new features that might be indicative of fraud. Examples include:\n",
        "        *   Transaction frequency per customer (`nameOrig`, `nameDest`) within a time window.\n",
        "        *   Average transaction amount per customer.\n",
        "        *   Ratio of transaction amount to old balance (`amount / oldbalanceOrg`).\n",
        "        *   Difference between old and new balances for both origin and destination accounts.\n",
        "        *   Indicators for sudden changes in transaction behavior.\n",
        "    *   **Anomaly Detection:** Explore unsupervised anomaly detection techniques for transactions that might not be labeled as fraud but exhibit unusual patterns.\n",
        "\n",
        "2.  **Time-Series Analysis:** The `step` column indicates time steps. Analyze transaction patterns over time. Are there specific steps (days) where fraud is more prevalent? Are certain `type` of transactions more common at certain `step` values?\n",
        "\n",
        "3.  **Graph Analysis:** The `nameOrig` and `nameDest` columns represent transacting entities. Construct a transaction network/graph to identify suspicious clusters or highly connected nodes that might be involved in fraudulent activities. Graph Neural Networks could be a powerful tool here.\n",
        "\n",
        "4.  **Data Quality and Missing Values:** Investigate the reason for missing values in `oldbalanceOrg`, `newbalanceOrig`, `nameDest`, `oldbalanceDest`, `newbalanceDest`, `isFraud`, and `isFlaggedFraud` (1 row missing). While minimal, understanding why these are missing could be important.\n",
        "\n",
        "### For `df_policy` (Compliance Analysis):\n",
        "\n",
        "1.  **Compliance Prediction Model:**\n",
        "    *   **Binary Classification:** The `Policy_Compliance` column (`Yes`/`No`) can be used as a target variable. Predict whether an employee will be compliant or non-compliant based on features like `Working_Days`, `Target_Sales`, `Actual_Sales`, `Customer_Satisfaction_Score`, `Low_Working_Days`, `Target_Not_Met`, and `Low_Customer_Satisfaction`.\n",
        "    *   **Feature Importance:** Determine which factors (e.g., `Target_Sales`, `Customer_Satisfaction_Score`) are most influential in predicting policy compliance.\n",
        "\n",
        "2.  **Root Cause Analysis for Non-Compliance:**\n",
        "    *   **Multi-label Classification/Association Rules:** The `Non_Compliance_Reason` column contains multiple reasons (e.g., \"Low Working Days, Low Customer Satisfaction\"). Analyze common combinations of reasons. This could be framed as a multi-label classification problem or by using association rule mining to find frequently co-occurring non-compliance reasons.\n",
        "    *   **Impact of `Month`:** Investigate if there's a seasonal or monthly trend in policy compliance or specific non-compliance reasons. (e.g., are employees less compliant in certain months?)\n",
        "\n",
        "3.  **Employee Performance vs. Compliance:** Analyze the relationship between performance metrics (`Actual_Sales`, `Customer_Satisfaction_Score`) and `Policy_Compliance`. Do high-performing employees tend to be more or less compliant?\n",
        "\n",
        "4.  **Clustering Employees:** Cluster employees based on their working patterns, sales performance, and compliance behaviors to identify different segments and tailor interventions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18e4d57c"
      },
      "source": [
        "## Suggested Further Analysis and Machine Learning Tasks\n",
        "\n",
        "Based on the initial data exploration of `df_transactions` (transaction log) and `df_policy` (policy compliance), here are some suggestions for further analysis and potential machine learning tasks:\n",
        "\n",
        "### For `df_transactions` (AML/Fraud Detection):\n",
        "\n",
        "1.  **Fraud Detection Model Development:**\n",
        "    *   **Binary Classification:** The `isFraud` column is a clear target variable. Develop a classification model (e.g., Logistic Regression, Random Forest, Gradient Boosting, Isolation Forest, or even Neural Networks) to predict fraudulent transactions. Given the highly imbalanced nature (`0.0`: 1,292,355 vs `1.0`: 1,548), techniques like SMOTE, ADASYN, or using appropriate evaluation metrics (Precision, Recall, F1-score, AUC-PR) will be crucial.\n",
        "    *   **Feature Engineering:** Create new features that might be indicative of fraud. Examples include:\n",
        "        *   Transaction frequency per customer (`nameOrig`, `nameDest`) within a time window.\n",
        "        *   Average transaction amount per customer.\n",
        "        *   Ratio of transaction amount to old balance (`amount / oldbalanceOrg`).\n",
        "        *   Difference between old and new balances for both origin and destination accounts.\n",
        "        *   Indicators for sudden changes in transaction behavior.\n",
        "    *   **Anomaly Detection:** Explore unsupervised anomaly detection techniques for transactions that might not be labeled as fraud but exhibit unusual patterns.\n",
        "\n",
        "2.  **Time-Series Analysis:** The `step` column indicates time steps. Analyze transaction patterns over time. Are there specific steps (days) where fraud is more prevalent? Are certain `type` of transactions more common at certain `step` values?\n",
        "\n",
        "3.  **Graph Analysis:** The `nameOrig` and `nameDest` columns represent transacting entities. Construct a transaction network/graph to identify suspicious clusters or highly connected nodes that might be involved in fraudulent activities. Graph Neural Networks could be a powerful tool here.\n",
        "\n",
        "4.  **Data Quality and Missing Values:** Investigate the reason for missing values in `oldbalanceOrg`, `newbalanceOrig`, `nameDest`, `oldbalanceDest`, `newbalanceDest`, `isFraud`, and `isFlaggedFraud` (1 row missing). While minimal, understanding why these are missing could be important.\n",
        "\n",
        "### For `df_policy` (Compliance Analysis):\n",
        "\n",
        "1.  **Compliance Prediction Model:**\n",
        "    *   **Binary Classification:** The `Policy_Compliance` column (`Yes`/`No`) can be used as a target variable. Predict whether an employee will be compliant or non-compliant based on features like `Working_Days`, `Target_Sales`, `Actual_Sales`, `Customer_Satisfaction_Score`, `Low_Working_Days`, `Target_Not_Met`, and `Low_Customer_Satisfaction`.\n",
        "    *   **Feature Importance:** Determine which factors (e.g., `Target_Sales`, `Customer_Satisfaction_Score`) are most influential in predicting policy compliance.\n",
        "\n",
        "2.  **Root Cause Analysis for Non-Compliance:**\n",
        "    *   **Multi-label Classification/Association Rules:** The `Non_Compliance_Reason` column contains multiple reasons (e.g., \"Low Working Days, Low Customer Satisfaction\"). Analyze common combinations of reasons. This could be framed as a multi-label classification problem or by using association rule mining to find frequently co-occurring non-compliance reasons.\n",
        "    *   **Impact of `Month`:** Investigate if there's a seasonal or monthly trend in policy compliance or specific non-compliance reasons. (e.g., are employees less compliant in certain months?)\n",
        "\n",
        "3.  **Employee Performance vs. Compliance:** Analyze the relationship between performance metrics (`Actual_Sales`, `Customer_Satisfaction_Score`) and `Policy_Compliance`. Do high-performing employees tend to be more or less compliant?\n",
        "\n",
        "4.  **Clustering Employees:** Cluster employees based on their working patterns, sales performance, and compliance behaviors to identify different segments and tailor interventions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7213022d"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the findings from the dataset inspection and provide recommendations for next steps based on your specific interests in anti-money laundering or compliance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7f6a3f6"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Transaction Log Dataset (`df_transactions`):**\n",
        "    *   The dataset contains 1,293,904 transactions across 11 columns.\n",
        "    *   A small number of missing values (1 entry) were identified in `oldbalanceOrg`, `newbalanceOrig`, `nameDest`, `oldbalanceDest`, `newbalanceDest`, `isFraud`, and `isFlaggedFraud`.\n",
        "    *   Transaction `amount` varies widely, ranging from \\$0.10 to \\$10,000,000.\n",
        "    *   A severe class imbalance exists in the `isFraud` column, with 1,292,355 (approximately 99.88%) non-fraudulent transactions ($0.0$) and only 1,548 (approximately 0.12%) fraudulent transactions ($1.0$).\n",
        "    *   The `isFlaggedFraud` column indicates that no transactions in this dataset were flagged as fraudulent by the system (all 1,293,903 entries are $0.0$).\n",
        "    *   The most frequent transaction types are `CASH_OUT` (460,441), `PAYMENT` (434,787), and `CASH_IN` (282,538).\n",
        "\n",
        "*   **Policy Compliance Dataset (`df_policy`):**\n",
        "    *   The dataset comprises 4,000 employee records with 12 columns.\n",
        "    *   The `Non_Compliance_Reason` column has 604 missing values, indicating that approximately 15.1% of employees have no specified reason for non-compliance, or they are compliant.\n",
        "    *   The `Policy_Compliance` column serves as a clear target variable, indicating whether an employee is compliant or not.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   **Address Severe Class Imbalance for Fraud Detection:** Given the extreme imbalance in the `isFraud` column, implementing advanced techniques like SMOTE, ADASYN, or using robust evaluation metrics (Precision, Recall, F1-score, AUC-PR) will be critical for developing effective fraud detection models.\n",
        "*   **Root Cause Analysis for Non-Compliance and Feature Engineering:** Investigate the patterns within the `Non_Compliance_Reason` column (e.g., common combinations, monthly trends) to understand the underlying causes of non-compliance. Concurrently, create new features from transaction data (e.g., transaction frequency, balance changes, and ratios) to improve the predictive power of fraud and compliance models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04bc8464"
      },
      "source": [
        "# Task\n",
        "Assess and handle missing values in `df_transactions` and `df_policy` by identifying and quantifying them, then applying appropriate strategies (imputation, deletion, or further investigation). After handling missing values, identify and remove duplicate rows in both DataFrames. Following this, review and optimize data types in `df_transactions` and `df_policy` by converting columns to more memory-efficient or appropriate types. Finally, provide a summary of all data cleaning steps and present the cleaned datasets, confirming their readiness for further analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e4c6409"
      },
      "source": [
        "## Assess and Handle Missing Values in df_transactions\n",
        "\n",
        "### Subtask:\n",
        "Identify and quantify missing values in the `df_transactions` DataFrame. Based on the analysis, decide on an appropriate strategy to handle these missing values (e.g., imputation, deletion, or further investigation), and apply it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "791fdd58"
      },
      "source": [
        "**Reasoning**:\n",
        "First, I will calculate and display the number of missing values for each column in `df_transactions` to understand the extent of the missing data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ca9ce7a",
        "outputId": "374a6ed9-b81b-4e40-e3e3-b79e570715d2"
      },
      "source": [
        "print(\"### Missing values in df_transactions (before handling) ###\")\n",
        "print(df_transactions.isnull().sum())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Missing values in df_transactions (before handling) ###\n",
            "step              0\n",
            "type              0\n",
            "amount            0\n",
            "nameOrig          0\n",
            "oldbalanceOrg     1\n",
            "newbalanceOrig    1\n",
            "nameDest          1\n",
            "oldbalanceDest    1\n",
            "newbalanceDest    1\n",
            "isFraud           1\n",
            "isFlaggedFraud    1\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "834a8b70"
      },
      "source": [
        "**Reasoning**:\n",
        "Since only one row contains missing values across multiple columns in `df_transactions`, I will remove this row using the `.dropna()` method to clean the dataset, as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4da08f8c",
        "outputId": "3f11f4b3-dc6f-4885-f9aa-15275e8e0708"
      },
      "source": [
        "df_transactions.dropna(inplace=True)\n",
        "print(\"### df_transactions - Missing values after dropping NA ###\")\n",
        "print(df_transactions.isnull().sum())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### df_transactions - Missing values after dropping NA ###\n",
            "step              0\n",
            "type              0\n",
            "amount            0\n",
            "nameOrig          0\n",
            "oldbalanceOrg     0\n",
            "newbalanceOrig    0\n",
            "nameDest          0\n",
            "oldbalanceDest    0\n",
            "newbalanceDest    0\n",
            "isFraud           0\n",
            "isFlaggedFraud    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0596d4c4"
      },
      "source": [
        "## Assess and Handle Missing Values in df_policy\n",
        "\n",
        "### Subtask:\n",
        "Identify and quantify missing values in the `df_policy` DataFrame. Based on the analysis, decide on an appropriate strategy to handle these missing values, especially considering the `Non_Compliance_Reason` column, and apply it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0730525"
      },
      "source": [
        "**Reasoning**:\n",
        "First, I will calculate and display the number of missing values for each column in `df_policy` to understand the extent of missing data, particularly for the `Non_Compliance_Reason` column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84f2d8c9",
        "outputId": "bac6df2a-3e25-42c9-cdef-ad7718752157"
      },
      "source": [
        "print(\"### Missing values in df_policy (before handling) ###\")\n",
        "print(df_policy.isnull().sum())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Missing values in df_policy (before handling) ###\n",
            "Employee_ID                      0\n",
            "Name                             0\n",
            "Working_Days                     0\n",
            "Target_Sales                     0\n",
            "Actual_Sales                     0\n",
            "Customer_Satisfaction_Score      0\n",
            "Policy_Compliance                0\n",
            "Low_Working_Days                 0\n",
            "Target_Not_Met                   0\n",
            "Low_Customer_Satisfaction        0\n",
            "Non_Compliance_Reason          604\n",
            "Month                            0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42537b69"
      },
      "source": [
        "**Reasoning**:\n",
        "I will impute the missing values in the `Non_Compliance_Reason` column based on the `Policy_Compliance` status, replacing 'NaN' with 'No Reason (Compliant)' where `Policy_Compliance` is 'Yes', and 'No Reason Specified' for other missing entries in `Non_Compliance_Reason`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65e85c7c",
        "outputId": "87618d15-fb61-4c2e-922c-e7c16651fad0"
      },
      "source": [
        "df_policy.loc[df_policy['Policy_Compliance'] == 'Yes', 'Non_Compliance_Reason'] = df_policy.loc[df_policy['Policy_Compliance'] == 'Yes', 'Non_Compliance_Reason'].fillna('No Reason (Compliant)')\n",
        "df_policy['Non_Compliance_Reason'].fillna('No Reason Specified', inplace=True)\n",
        "\n",
        "print(\"### Missing values in df_policy (after handling) ###\")\n",
        "print(df_policy.isnull().sum())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Missing values in df_policy (after handling) ###\n",
            "Employee_ID                    0\n",
            "Name                           0\n",
            "Working_Days                   0\n",
            "Target_Sales                   0\n",
            "Actual_Sales                   0\n",
            "Customer_Satisfaction_Score    0\n",
            "Policy_Compliance              0\n",
            "Low_Working_Days               0\n",
            "Target_Not_Met                 0\n",
            "Low_Customer_Satisfaction      0\n",
            "Non_Compliance_Reason          0\n",
            "Month                          0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1161526327.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_policy['Non_Compliance_Reason'].fillna('No Reason Specified', inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "338d3671"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code generated a `FutureWarning` due to using `inplace=True` in a chained assignment context. I will fix this by assigning the result of `fillna` directly back to the column and using a boolean mask with `.loc` for the conditional imputation, which is a more robust and future-proof way to handle this operation in pandas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f445cf91",
        "outputId": "196c3691-3725-49cc-efd8-1d246f851564"
      },
      "source": [
        "mask_compliant_and_nan = (df_policy['Policy_Compliance'] == 'Yes') & (df_policy['Non_Compliance_Reason'].isnull())\n",
        "df_policy.loc[mask_compliant_and_nan, 'Non_Compliance_Reason'] = 'No Reason (Compliant)'\n",
        "df_policy['Non_Compliance_Reason'] = df_policy['Non_Compliance_Reason'].fillna('No Reason Specified')\n",
        "\n",
        "print(\"### Missing values in df_policy (after handling) ###\")\n",
        "print(df_policy.isnull().sum())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Missing values in df_policy (after handling) ###\n",
            "Employee_ID                    0\n",
            "Name                           0\n",
            "Working_Days                   0\n",
            "Target_Sales                   0\n",
            "Actual_Sales                   0\n",
            "Customer_Satisfaction_Score    0\n",
            "Policy_Compliance              0\n",
            "Low_Working_Days               0\n",
            "Target_Not_Met                 0\n",
            "Low_Customer_Satisfaction      0\n",
            "Non_Compliance_Reason          0\n",
            "Month                          0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89a9b1b2"
      },
      "source": [
        "## Identify and Handle Duplicates in df_transactions\n",
        "\n",
        "### Subtask:\n",
        "Check for and quantify any duplicate rows in the `df_transactions` DataFrame. If duplicates are found, remove them to ensure data integrity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbb503ca"
      },
      "source": [
        "**Reasoning**:\n",
        "I will calculate and display the number of duplicate rows in `df_transactions` to identify if any exist before proceeding with removal.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87ed83ec",
        "outputId": "a9df222d-9f1e-48dc-a923-4281f916d862"
      },
      "source": [
        "print(\"### Duplicate rows in df_transactions (before handling) ###\")\n",
        "print(df_transactions.duplicated().sum())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Duplicate rows in df_transactions (before handling) ###\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e2d7741"
      },
      "source": [
        "**Reasoning**:\n",
        "Since the previous step showed no duplicate rows, no removal operation is needed. This step verifies that there are still no duplicates and concludes the handling of duplicates for `df_transactions`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f48c283",
        "outputId": "5c528be6-1ce8-41e7-d161-bdc591fa8708"
      },
      "source": [
        "print(\"### Duplicate rows in df_transactions (after handling) ###\")\n",
        "print(df_transactions.duplicated().sum())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Duplicate rows in df_transactions (after handling) ###\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69aa13b6"
      },
      "source": [
        "**Reasoning**:\n",
        "I will calculate and display the number of duplicate rows in `df_policy` to identify if any exist before proceeding with removal, similar to the process for `df_transactions`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b8b45ee",
        "outputId": "c184ce5e-60f8-4f97-c090-582d391b74ec"
      },
      "source": [
        "print(\"### Duplicate rows in df_policy (before handling) ###\")\n",
        "print(df_policy.duplicated().sum())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Duplicate rows in df_policy (before handling) ###\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f232aae0"
      },
      "source": [
        "**Reasoning**:\n",
        "Since no duplicate rows were found in `df_policy` in the previous step, no removal operation is needed. This step verifies that there are still no duplicates and concludes the handling of duplicates for `df_policy`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4161d5ec",
        "outputId": "c9ae178d-d58a-4594-dbc5-ab52fc9fa86e"
      },
      "source": [
        "print(\"### Duplicate rows in df_policy (after handling) ###\")\n",
        "print(df_policy.duplicated().sum())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Duplicate rows in df_policy (after handling) ###\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3f461a0"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that missing values and duplicate rows have been handled for both DataFrames, I will inspect the current data types of `df_transactions` to identify opportunities for optimization, as per the next instruction in the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11d26a91",
        "outputId": "35765f86-c100-418a-e60d-758769e57fc2"
      },
      "source": [
        "print(\"### df_transactions - Current Data Types ###\")\n",
        "print(df_transactions.info())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### df_transactions - Current Data Types ###\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 1293903 entries, 0 to 1293902\n",
            "Data columns (total 11 columns):\n",
            " #   Column          Non-Null Count    Dtype  \n",
            "---  ------          --------------    -----  \n",
            " 0   step            1293903 non-null  int64  \n",
            " 1   type            1293903 non-null  object \n",
            " 2   amount          1293903 non-null  float64\n",
            " 3   nameOrig        1293903 non-null  object \n",
            " 4   oldbalanceOrg   1293903 non-null  float64\n",
            " 5   newbalanceOrig  1293903 non-null  float64\n",
            " 6   nameDest        1293903 non-null  object \n",
            " 7   oldbalanceDest  1293903 non-null  float64\n",
            " 8   newbalanceDest  1293903 non-null  float64\n",
            " 9   isFraud         1293903 non-null  float64\n",
            " 10  isFlaggedFraud  1293903 non-null  float64\n",
            "dtypes: float64(7), int64(1), object(3)\n",
            "memory usage: 118.5+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd752d7f"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the `df_transactions.info()` output, I will optimize data types. `step` can be downcasted to `int16` as its maximum value is small. `type` can be converted to `category` as it has a limited number of unique values. `amount` and balance-related columns can be converted to `float32` to save memory without significant loss of precision. `isFraud` and `isFlaggedFraud` can be converted to `int8` as they are binary flags.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f298978",
        "outputId": "17922d9f-42b1-425c-ced8-a4d2e3c238b6"
      },
      "source": [
        "print(\"### Optimizing data types for df_transactions ###\")\n",
        "\n",
        "df_transactions['step'] = df_transactions['step'].astype('int16')\n",
        "df_transactions['type'] = df_transactions['type'].astype('category')\n",
        "df_transactions['amount'] = df_transactions['amount'].astype('float32')\n",
        "df_transactions['oldbalanceOrg'] = df_transactions['oldbalanceOrg'].astype('float32')\n",
        "df_transactions['newbalanceOrig'] = df_transactions['newbalanceOrig'].astype('float32')\n",
        "df_transactions['oldbalanceDest'] = df_transactions['oldbalanceDest'].astype('float32')\n",
        "df_transactions['newbalanceDest'] = df_transactions['newbalanceDest'].astype('float32')\n",
        "df_transactions['isFraud'] = df_transactions['isFraud'].astype('int8')\n",
        "df_transactions['isFlaggedFraud'] = df_transactions['isFlaggedFraud'].astype('int8')\n",
        "\n",
        "print(\"\\n### df_transactions - Data Types after Optimization ###\")\n",
        "df_transactions.info()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Optimizing data types for df_transactions ###\n",
            "\n",
            "### df_transactions - Data Types after Optimization ###\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 1293903 entries, 0 to 1293902\n",
            "Data columns (total 11 columns):\n",
            " #   Column          Non-Null Count    Dtype   \n",
            "---  ------          --------------    -----   \n",
            " 0   step            1293903 non-null  int16   \n",
            " 1   type            1293903 non-null  category\n",
            " 2   amount          1293903 non-null  float32 \n",
            " 3   nameOrig        1293903 non-null  object  \n",
            " 4   oldbalanceOrg   1293903 non-null  float32 \n",
            " 5   newbalanceOrig  1293903 non-null  float32 \n",
            " 6   nameDest        1293903 non-null  object  \n",
            " 7   oldbalanceDest  1293903 non-null  float32 \n",
            " 8   newbalanceDest  1293903 non-null  float32 \n",
            " 9   isFraud         1293903 non-null  int8    \n",
            " 10  isFlaggedFraud  1293903 non-null  int8    \n",
            "dtypes: category(1), float32(5), int16(1), int8(2), object(2)\n",
            "memory usage: 60.5+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ce558a4"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the data types for `df_transactions` have been optimized, I will inspect the current data types of `df_policy` to identify similar opportunities for optimization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1c00095",
        "outputId": "ae6735be-bb55-4b23-87fe-f921a3a1a723"
      },
      "source": [
        "print(\"### df_policy - Current Data Types ###\")\n",
        "print(df_policy.info())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### df_policy - Current Data Types ###\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4000 entries, 0 to 3999\n",
            "Data columns (total 12 columns):\n",
            " #   Column                       Non-Null Count  Dtype  \n",
            "---  ------                       --------------  -----  \n",
            " 0   Employee_ID                  4000 non-null   int64  \n",
            " 1   Name                         4000 non-null   object \n",
            " 2   Working_Days                 4000 non-null   int64  \n",
            " 3   Target_Sales                 4000 non-null   int64  \n",
            " 4   Actual_Sales                 4000 non-null   int64  \n",
            " 5   Customer_Satisfaction_Score  4000 non-null   float64\n",
            " 6   Policy_Compliance            4000 non-null   object \n",
            " 7   Low_Working_Days             4000 non-null   bool   \n",
            " 8   Target_Not_Met               4000 non-null   bool   \n",
            " 9   Low_Customer_Satisfaction    4000 non-null   bool   \n",
            " 10  Non_Compliance_Reason        4000 non-null   object \n",
            " 11  Month                        4000 non-null   object \n",
            "dtypes: bool(3), float64(1), int64(4), object(4)\n",
            "memory usage: 293.1+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d36386e8"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the `df_policy.info()` output, I will optimize data types. `Employee_ID`, `Working_Days`, `Target_Sales`, and `Actual_Sales` can be downcasted to smaller integer types. `Customer_Satisfaction_Score` can be converted to `float32`. `Policy_Compliance`, `Non_Compliance_Reason`, and `Month` can be converted to `category` type to save memory, as they have a limited number of unique values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edd1e4a9",
        "outputId": "8b14a4f3-6b38-4e28-a693-568ed5d0cac4"
      },
      "source": [
        "print(\"### Optimizing data types for df_policy ###\")\n",
        "\n",
        "df_policy['Employee_ID'] = df_policy['Employee_ID'].astype('int16')\n",
        "df_policy['Working_Days'] = df_policy['Working_Days'].astype('int8')\n",
        "df_policy['Target_Sales'] = df_policy['Target_Sales'].astype('int16')\n",
        "df_policy['Actual_Sales'] = df_policy['Actual_Sales'].astype('int16')\n",
        "df_policy['Customer_Satisfaction_Score'] = df_policy['Customer_Satisfaction_Score'].astype('float32')\n",
        "df_policy['Policy_Compliance'] = df_policy['Policy_Compliance'].astype('category')\n",
        "df_policy['Non_Compliance_Reason'] = df_policy['Non_Compliance_Reason'].astype('category')\n",
        "df_policy['Month'] = df_policy['Month'].astype('category')\n",
        "\n",
        "print(\"\\n### df_policy - Data Types after Optimization ###\")\n",
        "df_policy.info()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Optimizing data types for df_policy ###\n",
            "\n",
            "### df_policy - Data Types after Optimization ###\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4000 entries, 0 to 3999\n",
            "Data columns (total 12 columns):\n",
            " #   Column                       Non-Null Count  Dtype   \n",
            "---  ------                       --------------  -----   \n",
            " 0   Employee_ID                  4000 non-null   int16   \n",
            " 1   Name                         4000 non-null   object  \n",
            " 2   Working_Days                 4000 non-null   int8    \n",
            " 3   Target_Sales                 4000 non-null   int16   \n",
            " 4   Actual_Sales                 4000 non-null   int16   \n",
            " 5   Customer_Satisfaction_Score  4000 non-null   float32 \n",
            " 6   Policy_Compliance            4000 non-null   category\n",
            " 7   Low_Working_Days             4000 non-null   bool    \n",
            " 8   Target_Not_Met               4000 non-null   bool    \n",
            " 9   Low_Customer_Satisfaction    4000 non-null   bool    \n",
            " 10  Non_Compliance_Reason        4000 non-null   category\n",
            " 11  Month                        4000 non-null   category\n",
            "dtypes: bool(3), category(3), float32(1), int16(3), int8(1), object(1)\n",
            "memory usage: 98.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e16e1418"
      },
      "source": [
        "## Summary of Data Cleaning Steps\n",
        "\n",
        "### df_transactions:\n",
        "1.  **Missing Values:** Identified and removed 1 row with missing values across multiple columns (`oldbalanceOrg`, `newbalanceOrig`, `nameDest`, `oldbalanceDest`, `newbalanceDest`, `isFraud`, `isFlaggedFraud`).\n",
        "2.  **Duplicate Rows:** Checked for duplicate rows; no duplicates were found.\n",
        "3.  **Data Type Optimization:**\n",
        "    *   `step`: Downcasted from `int64` to `int16`.\n",
        "    *   `type`: Converted from `object` to `category`.\n",
        "    *   `amount`, `oldbalanceOrg`, `newbalanceOrig`, `oldbalanceDest`, `newbalanceDest`: Converted from `float64` to `float32`.\n",
        "    *   `isFraud`, `isFlaggedFraud`: Converted from `float64` to `int8`.\n",
        "\n",
        "### df_policy:\n",
        "1.  **Missing Values:** Imputed missing values in `Non_Compliance_Reason`. Entries where `Policy_Compliance` was 'Yes' were filled with 'No Reason (Compliant)', and other missing entries were filled with 'No Reason Specified'.\n",
        "2.  **Duplicate Rows:** Checked for duplicate rows; no duplicates were found.\n",
        "3.  **Data Type Optimization:**\n",
        "    *   `Employee_ID`, `Target_Sales`, `Actual_Sales`: Downcasted from `int64` to `int16`.\n",
        "    *   `Working_Days`: Downcasted from `int64` to `int8`.\n",
        "    *   `Customer_Satisfaction_Score`: Converted from `float64` to `float32`.\n",
        "    *   `Policy_Compliance`, `Non_Compliance_Reason`, `Month`: Converted from `object` to `category`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56819145"
      },
      "source": [
        "**Reasoning**:\n",
        "To present the cleaned datasets and confirm their readiness for further analysis, I will display the first few rows of both `df_transactions` and `df_policy`, along with their `info()` output, which shows the updated data types and non-null counts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f25b88de",
        "outputId": "39c4ee52-4b4c-403c-bb7e-23b377ed8b0a"
      },
      "source": [
        "print(\"\\n### df_transactions - Cleaned Data Overview ###\")\n",
        "print(df_transactions.head())\n",
        "df_transactions.info()\n",
        "\n",
        "print(\"\\n### df_policy - Cleaned Data Overview ###\")\n",
        "print(df_policy.head())\n",
        "df_policy.info()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### df_transactions - Cleaned Data Overview ###\n",
            "   step      type        amount     nameOrig  oldbalanceOrg  newbalanceOrig  \\\n",
            "0     1   PAYMENT   9839.639648  C1231006815       170136.0   160296.359375   \n",
            "1     1   PAYMENT   1864.280029  C1666544295        21249.0    19384.720703   \n",
            "2     1  TRANSFER    181.000000  C1305486145          181.0        0.000000   \n",
            "3     1  CASH_OUT    181.000000   C840083671          181.0        0.000000   \n",
            "4     1   PAYMENT  11668.139648  C2048537720        41554.0    29885.859375   \n",
            "\n",
            "      nameDest  oldbalanceDest  newbalanceDest  isFraud  isFlaggedFraud  \n",
            "0  M1979787155             0.0             0.0        0               0  \n",
            "1  M2044282225             0.0             0.0        0               0  \n",
            "2   C553264065             0.0             0.0        1               0  \n",
            "3    C38997010         21182.0             0.0        1               0  \n",
            "4  M1230701703             0.0             0.0        0               0  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 1293903 entries, 0 to 1293902\n",
            "Data columns (total 11 columns):\n",
            " #   Column          Non-Null Count    Dtype   \n",
            "---  ------          --------------    -----   \n",
            " 0   step            1293903 non-null  int16   \n",
            " 1   type            1293903 non-null  category\n",
            " 2   amount          1293903 non-null  float32 \n",
            " 3   nameOrig        1293903 non-null  object  \n",
            " 4   oldbalanceOrg   1293903 non-null  float32 \n",
            " 5   newbalanceOrig  1293903 non-null  float32 \n",
            " 6   nameDest        1293903 non-null  object  \n",
            " 7   oldbalanceDest  1293903 non-null  float32 \n",
            " 8   newbalanceDest  1293903 non-null  float32 \n",
            " 9   isFraud         1293903 non-null  int8    \n",
            " 10  isFlaggedFraud  1293903 non-null  int8    \n",
            "dtypes: category(1), float32(5), int16(1), int8(2), object(2)\n",
            "memory usage: 60.5+ MB\n",
            "\n",
            "### df_policy - Cleaned Data Overview ###\n",
            "   Employee_ID         Name  Working_Days  Target_Sales  Actual_Sales  \\\n",
            "0            1  Ahmed Tariq            21         14435         19470   \n",
            "1            2  Usman Iqbal            18          9998          9968   \n",
            "2            3  Fatima Khan            27          5162         11493   \n",
            "3            4   Zain Tariq            29          6974          8103   \n",
            "4            5  Ahmed Malik            25          8291          6076   \n",
            "\n",
            "   Customer_Satisfaction_Score Policy_Compliance  Low_Working_Days  \\\n",
            "0                          2.3                No             False   \n",
            "1                          3.0                No              True   \n",
            "2                          3.3                No             False   \n",
            "3                          2.8                No             False   \n",
            "4                          3.7                No             False   \n",
            "\n",
            "   Target_Not_Met  Low_Customer_Satisfaction  \\\n",
            "0           False                       True   \n",
            "1           False                       True   \n",
            "2           False                       True   \n",
            "3           False                       True   \n",
            "4            True                       True   \n",
            "\n",
            "                         Non_Compliance_Reason     Month  \n",
            "0                    Low Customer Satisfaction     March  \n",
            "1  Low Working Days, Low Customer Satisfaction  December  \n",
            "2                    Low Customer Satisfaction  February  \n",
            "3                    Low Customer Satisfaction       May  \n",
            "4    Target Not Met, Low Customer Satisfaction  February  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4000 entries, 0 to 3999\n",
            "Data columns (total 12 columns):\n",
            " #   Column                       Non-Null Count  Dtype   \n",
            "---  ------                       --------------  -----   \n",
            " 0   Employee_ID                  4000 non-null   int16   \n",
            " 1   Name                         4000 non-null   object  \n",
            " 2   Working_Days                 4000 non-null   int8    \n",
            " 3   Target_Sales                 4000 non-null   int16   \n",
            " 4   Actual_Sales                 4000 non-null   int16   \n",
            " 5   Customer_Satisfaction_Score  4000 non-null   float32 \n",
            " 6   Policy_Compliance            4000 non-null   category\n",
            " 7   Low_Working_Days             4000 non-null   bool    \n",
            " 8   Target_Not_Met               4000 non-null   bool    \n",
            " 9   Low_Customer_Satisfaction    4000 non-null   bool    \n",
            " 10  Non_Compliance_Reason        4000 non-null   category\n",
            " 11  Month                        4000 non-null   category\n",
            "dtypes: bool(3), category(3), float32(1), int16(3), int8(1), object(1)\n",
            "memory usage: 98.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f580f79d"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "*   **How were missing values handled in `df_transactions` and `df_policy`?**\n",
        "    In `df_transactions`, a single row containing missing values across seven columns (`oldbalanceOrg`, `newbalanceOrig`, `nameDest`, `oldbalanceDest`, `newbalanceDest`, `isFraud`, and `isFlaggedFraud`) was identified and removed. For `df_policy`, 604 missing values in the `Non_Compliance_Reason` column were handled by conditional imputation: rows where `Policy_Compliance` was 'Yes' had `Non_Compliance_Reason` filled with 'No Reason (Compliant)', and all other remaining missing entries were filled with 'No Reason Specified'.\n",
        "*   **Were duplicate rows found and removed from `df_transactions` and `df_policy`?**\n",
        "    No duplicate rows were found in either `df_transactions` or `df_policy`, so no removal was necessary.\n",
        "*   **How were data types optimized for `df_transactions` and `df_policy`?**\n",
        "    For `df_transactions`, columns like `step` were downcasted to `int16`, `type` was converted to `category`, numerical columns such as `amount` were converted from `float64` to `float32`, and boolean-like columns (`isFraud`, `isFlaggedFraud`) were converted from `float64` to `int8`. In `df_policy`, `Employee_ID`, `Target_Sales`, and `Actual_Sales` were downcasted to `int16`, `Working_Days` to `int8`, `Customer_Satisfaction_Score` to `float32`, and categorical columns like `Policy_Compliance`, `Non_Compliance_Reason`, and `Month` were converted to the `category` type.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   `df_transactions` initially contained 1 row with missing values across 7 columns; this row was successfully removed, resulting in zero missing values.\n",
        "*   `df_policy` had 604 missing values exclusively in the `Non_Compliance_Reason` column, which were successfully imputed using a conditional strategy, leaving no missing values in the DataFrame.\n",
        "*   No duplicate rows were found in either `df_transactions` or `df_policy` after initial checks.\n",
        "*   Data type optimization significantly reduced memory usage:\n",
        "    *   `df_transactions` memory usage decreased from over 118.5 MB to 60.5 MB.\n",
        "    *   `df_policy` memory usage decreased from over 293.1 KB to 98.6 KB.\n",
        "*   Specific data type changes included converting `object` types to `category` for improved memory and performance, downcasting integer types to more appropriate smaller integer types (e.g., `int64` to `int16`), and converting `float64` to `float32` for numerical columns.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   Both `df_transactions` and `df_policy` are now thoroughly cleaned, free of missing values and duplicates, and have optimized data types, making them ready for more advanced analytical tasks.\n",
        "*   The substantial memory reduction achieved through data type optimization will improve computational efficiency for subsequent analysis, especially beneficial for larger datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "081e391f"
      },
      "source": [
        "# Task\n",
        "Confirm that duplicate handling for `df_policy` was completed and the dataset remains free of duplicates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "750e5b5e"
      },
      "source": [
        "## Verify No Duplicates in df_policy\n",
        "\n",
        "### Subtask:\n",
        "Re-run the check for duplicate rows in `df_policy` to confirm that no duplicates are present after the cleaning steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a0402b7"
      },
      "source": [
        "**Reasoning**:\n",
        "I will re-check for duplicate rows in `df_policy` using the `.duplicated().sum()` method to confirm that no new duplicates have been introduced or remained after previous cleaning steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e939d8bb",
        "outputId": "cdd46b58-0494-47cd-d7fe-e6194960cd1c"
      },
      "source": [
        "print(\"### Duplicate rows in df_policy (after all cleaning steps) ###\")\n",
        "print(df_policy.duplicated().sum())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Duplicate rows in df_policy (after all cleaning steps) ###\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5897372f"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Confirm that duplicate handling for `df_policy` was completed and the dataset remains free of duplicates.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a687dd42"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "Yes, duplicate handling for `df_policy` was completed, and the dataset remains free of duplicates.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   After all cleaning steps, the `df_policy` DataFrame contains 0 duplicate rows.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The `df_policy` dataset has been successfully cleaned of duplicates, ensuring data integrity for subsequent analyses.\n"
      ]
    }
  ]
}